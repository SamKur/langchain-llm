{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Prompt UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gradio streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "model = ChatAnthropic(model='claude-3-haiku-20240307')\n",
    "\n",
    "import streamlit as st\n",
    "user_input = st.text_input('Enter your prompt')\n",
    "if st.button('Summarize Reseach Paper'):\n",
    "    result = model.invoke(user_input)   # result = model.invoke('Hello world')\n",
    "    st.write(result.content)            # print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8LDcu9fpX6b"
   },
   "source": [
    "## Dynamic prompt - using PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1w7SGIP5lTi"
   },
   "outputs": [],
   "source": [
    "# why not fstring\n",
    "# 1. code validation\n",
    "# 2. re-use by save-load in json format\n",
    "# 3. langchain ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1D-iOYaH6u8A"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, load_prompt\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo')\n",
    "\n",
    "product1 = input()   # or choose from List by steamlit dropdown\n",
    "template = PromptTemplate(template=\"\"\"\n",
    "I want you to act as a naming consultant for new companies.\n",
    "What is a good name for a company that makes {product}?\"\"\", input_variables=['product'], validate_template= True)\n",
    "\n",
    "prompt = template.invoke({'product':product1}) # if input_variables=[] here send empty dict {}\n",
    "result = model.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKaHoxcoEPjt"
   },
   "outputs": [],
   "source": [
    "# OR\n",
    "chain = template | model\n",
    "result = chain.invoke({'product':product1})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrO73lexAQ_r"
   },
   "outputs": [],
   "source": [
    "template.save('templates/01_template_for_name.json')\n",
    "# loaded_ptemplate = load_prompt('templates/01_template_for_name.json') # load by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPzi7YA2GAEG"
   },
   "source": [
    "## Create chatbot - terminal based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-NTTIH9GB2w"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yxDFdbzTIORf"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input == 'quit':\n",
    "        break\n",
    "    result = model.invoke(user_input)\n",
    "    print(\"AI:\", result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aP7eTr65JxX2"
   },
   "outputs": [],
   "source": [
    "# the above code is all okay but \n",
    "# 1. can't remember early conversations (and who told what)\n",
    "# 2. no template used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzpX-OJ-L9gq"
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    chat_history.append(user_input)\n",
    "    if user_input == 'quit':\n",
    "        break\n",
    "    result = model.invoke(chat_history) # not only user_input\n",
    "    chat_history.append(result.content)\n",
    "    print(\"AI: \", result.content)\n",
    "\n",
    "# print(\"ENTIRE CHAT STORED:\")\n",
    "# print(*chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHbxvA9wgjv8"
   },
   "outputs": [],
   "source": [
    "# above we easily cant differentiate which one is AI msg or USR msg, so use `messages`\n",
    "\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "model= ChatOpenAI(model_name='gpt-3.5-turbo')\n",
    "chat_history = [SystemMessage(content=\"You are a doctor AI bot.\")] # FIXED system message\n",
    "                                    # then append Human/AI msg accordingly\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    chat_history.append(HumanMessage(content=user_input)) # no template used\n",
    "    if user_input == 'quit':\n",
    "        break\n",
    "    result = model.invoke(chat_history) # NB\n",
    "    chat_history.append(AIMessage(content=result.content))\n",
    "    print(result.content)\n",
    "\n",
    "# print(\"ENTIRE CHAT STORED:\")\n",
    "# print(*chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K09-qMz0-qqj"
   },
   "source": [
    "## Template for reusable, parameterized initial prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1746449186412,
     "user": {
      "displayName": "Susamay Kumbhakar",
      "userId": "17628977713333455389"
     },
     "user_tz": -330
    },
    "id": "Ec1vNlsR_O8u",
    "outputId": "79f84ad5-4979-414f-c64a-e94bfd26f84c"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# Using ChatPromptTemplate([...]) instead of ChatPromptTemplate.from_messages according to latest docs\n",
    "chat_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful {domain} AI bot.\"),\n",
    "    (\"human\", \"explain in simple terms about {topic}\"),\n",
    "                                                            # how AIMessage? => \"ai\"\n",
    "])\n",
    "\n",
    "prompt = chat_template.invoke({'domain':'medical', 'topic':'type2 diabetes'})\n",
    "print(prompt)\n",
    "\n",
    "# then the entire code to get content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dIA2RO_jbiP"
   },
   "source": [
    "## MessagesPlaceholder\n",
    "Loading older chats from db/txt/blob_storage to ChatPromptTemplate\n",
    "### Full fleged-use of templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1746452195688,
     "user": {
      "displayName": "Susamay Kumbhakar",
      "userId": "17628977713333455389"
     },
     "user_tz": -330
    },
    "id": "w-e_rg9EoNeB"
   },
   "outputs": [],
   "source": [
    "# creating synthetic txt file as history\n",
    "\n",
    "with open('04_chat_history.txt', 'w') as f:\n",
    "    f.write('HumanMessage(content=\"I want to request a refund for my order #12345.\")\\n')\n",
    "    f.write('AIMessage(content=\"Your refund request for order #12345 has been initiated. It will be processed in 3-5 business days.\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94,
     "status": "ok",
     "timestamp": 1746455800392,
     "user": {
      "displayName": "Susamay Kumbhakar",
      "userId": "17628977713333455389"
     },
     "user_tz": -330
    },
    "id": "ELtrknGNjxrd",
    "outputId": "392cd9b4-5748-488c-86e6-84d91cda1642"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# chat template\n",
    "chat_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful customer support agent.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"), # here history is shared\n",
    "    (\"human\", \"{query}\")\n",
    "])\n",
    "\n",
    "# load chat history\n",
    "chat_history = []\n",
    "with open('04_chat_history.txt', 'r') as f:\n",
    "    # chat_history.extend(f.readlines()) # naive way, not recommended\n",
    "    # below code is verbose and manual work needed because of not-so-mature ðŸ¦œðŸ”— ecosystem\n",
    "    for line in f:\n",
    "        if 'HumanMessage' in line:\n",
    "            content = line.split('content=\"')[1].split('\"')[0]\n",
    "            chat_history.append(HumanMessage(content=content))\n",
    "        elif 'AIMessage' in line:\n",
    "            content = line.split('content=\"')[1].split('\"')[0]\n",
    "            chat_history.append(AIMessage(content=content))\n",
    "# print(chat_history)\n",
    "\n",
    "# create prompt\n",
    "prompt = chat_template.invoke({'chat_history':chat_history, 'query':'where is my order?'})\n",
    "print(prompt)\n",
    "\n",
    "# the next as is\n",
    "result = model.invoke(prompt)\n",
    "print(result.text) # we can store these too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using gradio to showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "# Init LLM\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "\n",
    "# Chat template\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful customer support agent.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{query}\")\n",
    "])\n",
    "\n",
    "# Load chat history (optional)\n",
    "chat_history = []\n",
    "if os.path.exists('04_chat_history.txt'):\n",
    "    with open('04_chat_history.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            if 'HumanMessage' in line:\n",
    "                content = line.split('content=\"')[1].split('\"')[0]\n",
    "                chat_history.append(HumanMessage(content=content))\n",
    "            elif 'AIMessage' in line:\n",
    "                content = line.split('content=\"')[1].split('\"')[0]\n",
    "                chat_history.append(AIMessage(content=content))\n",
    "\n",
    "# Chat function\n",
    "def chat_fn(query, history_ui):\n",
    "    global chat_history\n",
    "\n",
    "    # Build prompt using LangChain\n",
    "    prompt = chat_template.invoke({\n",
    "        'chat_history': chat_history,\n",
    "        'query': query\n",
    "    })\n",
    "\n",
    "    # Get response from OpenAI\n",
    "    response = model.invoke(prompt).content\n",
    "\n",
    "    # Update in-memory history\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "    chat_history.append(AIMessage(content=response))\n",
    "\n",
    "    # Save to file\n",
    "    with open(\"04_chat_history.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f'HumanMessage content=\"{query}\"\\n')\n",
    "        f.write(f'AIMessage content=\"{response}\"\\n')\n",
    "\n",
    "    return response  # Gradio expects a string here\n",
    "\n",
    "# Gradio UI\n",
    "gr.ChatInterface(\n",
    "    fn=chat_fn,\n",
    "    chatbot=gr.Chatbot(),\n",
    "    textbox=gr.Textbox(placeholder=\"Ask your question...\", show_label=False),\n",
    "    title=\"Customer Support Chatbot\"\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evDfMagazq9Y"
   },
   "outputs": [],
   "source": [
    "## more prompting techniques - few shot template, chain of thought, gk"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaFmCPQ5FdQmqVuS2XZAOk",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lang_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
