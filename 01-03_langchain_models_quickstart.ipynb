{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either by directly passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
    "# .env file content\n",
    "OPENAI_API_KEY = \"\"\n",
    "ANTHROPIC_API_KEY = \"\"\n",
    "GOOGLE_API_KEY = \"\"\n",
    "HUGGINGFACEHUB_ACCESS_TOKEN = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "import os\n",
    "# print( os.getenv('GOOGLE_API_KEY') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct client call\n",
    "```%pip install openai anthropic google-genai```\n",
    "\n",
    "[API Pricing Compare](https://yourgpt.ai/tools/openai-and-other-llm-api-pricing-calculator)\n",
    "\n",
    "[OpenAI Models List](https://platform.openai.com/docs/models#:~:text=Cost%2Doptimized%20models)\n",
    "\n",
    "[Anthropic Models List](https://docs.anthropic.com/en/docs/about-claude/models/overview#model-aliases)\n",
    "\n",
    "[Google Models List](https://ai.google.dev/gemini-api/docs/models#model-variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGxzGxjGlGiG"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "completion = client.chat.completions.create(model='gpt-4.1-nano', messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdBK7UsLl0AF"
   },
   "outputs": [],
   "source": [
    "import anthropic\n",
    "client = anthropic.Anthropic()\n",
    "message = client.messages.create(model='claude-3-haiku-20240307', messages=[{\"role\": \"user\", \"content\": \"Hello world\"}], max_tokens=100)\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "client = genai.Client()\n",
    "response = client.models.generate_content(model=\"gemini-2.0-flash\", contents=\"Hello world\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K52uzuyZmsGA"
   },
   "source": [
    "## Using langchain unified interface for Models\n",
    "```%pip install langchain_openai langchain_anthropic langchain-google-genai```\n",
    "\n",
    "[OpenAI API Ref](https://python.langchain.com/api_reference/openai/index.html) / [OpenAI Integration Docs](https://python.langchain.com/docs/integrations/providers/openai/)\n",
    "\n",
    "[Anthropic API Ref](https://python.langchain.com/api_reference/anthropic/index.html) / [Anthropic Integration Docs](https://python.langchain.com/docs/integrations/providers/anthropic/)\n",
    "\n",
    "[Google API Ref](https://python.langchain.com/api_reference/google_genai/index.html#module-langchain_google_genai) / [Google Integration Docs](https://python.langchain.com/docs/integrations/providers/google/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HX__zey1m7VE"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI # they are chat models (not LLM or embedding models)\n",
    "model = ChatOpenAI(model='gpt-4.1-nano')# optional temperature 0~2 -> measure of randomness; top_p -> diverse word collection\n",
    "                                        # max_completion_tokens -> op token size, restrict for lower api usage cost\n",
    "result = model.invoke('Hello world')\n",
    "print(result)\n",
    "print(result.content)                   # answer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtXO-1_wnnd9"
   },
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "model = ChatAnthropic(model='claude-3-haiku-20240307')\n",
    "result = model.invoke('Hello world')\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5R6fW7fRkJ4V"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(model='gemini-2.0-flash')\n",
    "result = model.invoke('Hello world')\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NToHigt3f2e7"
   },
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "llm = HuggingFaceEndpoint(repo_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", task=\"text-generation\") # EXTRA: model_kwargs={\"temperature\": 0.3, \"max_length\": 50}\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "result = model.invoke(\"Who is first PM of India? Answer in two words.\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7USQjv9jQup"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = 'D:/huggingface/models' # optional\n",
    "\n",
    "# for hf in local - Pipeline\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "llm=HuggingFacePipeline.from_model_id(  model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", task=\"text-generation\",\n",
    "                                        pipeline_kwargs={\"temperature\": 0.3, \"max_length\": 50}  )\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "result = model.invoke(\"What are the 3 parts of CPU?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgn3wJZAmlmf"
   },
   "source": [
    "## Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrLyhlZKmpiD"
   },
   "outputs": [],
   "source": [
    "# OpenAI Embeddings - for single query\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnacHIbJm-BL"
   },
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=32) # vector dim of 32 items, greater captures more context\n",
    "result = embedding.embed_query(\"3 parts of CPU are ALU, CU, MU.\")\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_msiC5dNtI5y"
   },
   "outputs": [],
   "source": [
    "# OpenAI Embeddings - for docs/texts (multiple query)\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\", dimensions=128)\n",
    "texts = [\"Who are you?\", \"Whats the full form of WHO?\", \"What is full moon\"]\n",
    "result = embedding.embed_documents(texts)\n",
    "print(str(result))   # size (3,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbWDg4nH4Lfm"
   },
   "outputs": [],
   "source": [
    "# HF local embeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "text = \"This is a test document.\"\n",
    "result = embedding.embed_query(text) # or .embed_documents(texts)\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I65aax5e_tXe"
   },
   "source": [
    "## Document Similarity - Nano Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1746247076453,
     "user": {
      "displayName": "Susamay Kumbhakar",
      "userId": "17628977713333455389"
     },
     "user_tz": -330
    },
    "id": "a220Lrev94L9"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJidDuiyE64J"
   },
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(model='text-embedding-3-large', dimension=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 91,
     "status": "ok",
     "timestamp": 1746247084117,
     "user": {
      "displayName": "Susamay Kumbhakar",
      "userId": "17628977713333455389"
     },
     "user_tz": -330
    },
    "id": "SGq4qJKwH80T"
   },
   "outputs": [],
   "source": [
    "documents = [\"...\",\"...\",\"...\",\"...\",\"...\"] # like related to indian cricket team members\n",
    "query = \"Tell me about Virat Kohli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1746247085188,
     "user": {
      "displayName": "Susamay Kumbhakar",
      "userId": "17628977713333455389"
     },
     "user_tz": -330
    },
    "id": "8q-6kk7QItmM"
   },
   "outputs": [],
   "source": [
    "doc_embeddings = embedding.embed_documents(documents) # 5 vectors, each have 300 dim\n",
    "query_embedding = embedding.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xdxFZZwMnLA"
   },
   "outputs": [],
   "source": [
    "cosine_similarities = cosine_similarity([query_embedding], doc_embeddings) # both input should be 2D list, output also 2D\n",
    "similarity = cosine_similarities[0] # [ , , , , ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "executionInfo": {
     "elapsed": 67,
     "status": "error",
     "timestamp": 1746248154231,
     "user": {
      "displayName": "Susamay Kumbhakar",
      "userId": "17628977713333455389"
     },
     "user_tz": -330
    },
    "id": "3X9vNoI5cYrc",
    "outputId": "de6fd883-1371-4d87-b5a7-31e3b93ad87e"
   },
   "outputs": [],
   "source": [
    "index, score = sorted(  list(enumerate(similarity)) , key=lambda x:x[1]  )[-1]\n",
    "index, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f01Dt5oBgDmP"
   },
   "outputs": [],
   "source": [
    "print(documents[index]) # the highest likely document\n",
    "print(score) # respective similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP3ercPMp5Bdv9K48eSpgE1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lang_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
