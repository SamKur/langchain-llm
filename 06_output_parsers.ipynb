{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a48709",
   "metadata": {},
   "source": [
    "## OutputParsers\n",
    "\n",
    "`langchain.output_parsers` Most user-facing / ready-to-use AND most plug-and-play use cases âœ…\n",
    "\n",
    "and\n",
    "\n",
    "`langchain_core.output_parsers` Core abstractions & base APIs OR workflows with LCEL (Runnable chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b1277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf llm model \n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# define the model\n",
    "llm = HuggingFaceEndpoint(repo_id=\"google/gemma-2-2b-it\", task=\"text-generation\")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "# 1st prompt -> detailed report\n",
    "template1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "# 2nd prompt -> summary\n",
    "template2 = PromptTemplate(\n",
    "    template='Write a 5 point summary on the following text. /n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "prompt1 = template1.invoke({'topic':'black hole'})\n",
    "\n",
    "result = model.invoke(prompt1)\n",
    "\n",
    "prompt2 = template2.invoke({'text':result.content}) # can't directly chain because of result.content. \n",
    "        # fix -> StrOutputParser &&  \n",
    "        # chain = template1 | model | parser | template2 | model | parser && chain.invoke({'topic':...})\n",
    "\n",
    "result1 = model.invoke(prompt2)\n",
    "\n",
    "print(result1.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4687773",
   "metadata": {},
   "source": [
    "### StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30beebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# 1st prompt -> detailed report\n",
    "template1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "# 2nd prompt -> summary\n",
    "template2 = PromptTemplate(\n",
    "    template='Write a 5 point summary on the following text. /n {text}',\n",
    "    input_variables=['text'] # if here multiple input variable then RunnablePassthrough is needed in chain\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = template1 | model | parser | template2 | model | parser\n",
    "\n",
    "result = chain.invoke({'topic':'alpha centauri 200'})\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb397cb",
   "metadata": {},
   "source": [
    "### Using RunnablePassthrough \n",
    "#### for multiple input variables for the subsequent prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough # Needed for more complex chaining\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=\"google/gemma-2-2b-it\", task=\"text-generation\")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "parser = StrOutputParser() # Using 'parser' for brevity\n",
    "\n",
    "# 1st prompt -> detailed report\n",
    "template1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "# 2nd prompt -> summary\n",
    "template2 = PromptTemplate(\n",
    "    template='Write a 5 point summary on the following text. \\n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "# The correct way to build the single chain\n",
    "full_chain = (\n",
    "    # Step 1: Input comes in as {'topic': '...'}.\n",
    "    # We want to use this 'topic' for template1.\n",
    "    # We also want the *output* of template1|model|parser to be assigned to 'text' for the next step.\n",
    "    {\n",
    "        \"text\": template1 | model | parser, # This runs the first part of the chain and assigns its output to 'text'\n",
    "        \"topic\": RunnablePassthrough() # This passes the original 'topic' input through, if you needed it later\n",
    "    }\n",
    "    # Step 2: Now the input to the next stage is a dictionary like {'text': 'report...', 'topic': {'topic': 'original_topic'}}.\n",
    "    # We only need the 'text' for template2.\n",
    "    | {\n",
    "        \"text\": lambda x: x[\"text\"] # Extract the 'text' (detailed report) from the previous step's output\n",
    "    }\n",
    "    # Step 3: Apply the second template\n",
    "    | template2\n",
    "    # Step 4: Pass to the model\n",
    "    | model\n",
    "    # Step 5: Parse the final output\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Invoke the single chain with the initial topic\n",
    "final_summary = full_chain.invoke({'topic': 'black hole'})\n",
    "\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42e31a",
   "metadata": {},
   "source": [
    "### JsonOutputParser\n",
    "#### Customizable parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caaadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser, XMLOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=\"google/gemma-2-2b-it\", task=\"text-generation\")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "# parser = XMLOutputParser()\n",
    "# parser -> SimpleJsonOutputParser PydanticOutputParser XMLOutputParser, MarkdownListOutputParser etc etc\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Give me 3 mind-blowing facts about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction': parser.get_format_instructions()} # before runtime\n",
    ")\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "result = chain.invoke({'topic':'UY Scuti'})\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec657fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = XMLOutputParser()\n",
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ad7b7",
   "metadata": {},
   "source": [
    "### StructuredOutputParser - gives result following json or pydantic schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2698d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema # N.B.\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=\"google/gemma-2-2b-it\", task=\"text-generation\")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "schema = [\n",
    "    ResponseSchema(name='fact_1', description='Fact 1 about the topic', type=\"string\"),\n",
    "    ResponseSchema(name='fact_2', description='Fact 2 about the topic'),\n",
    "    ResponseSchema(name='fact_3', description='Fact 3 about the topic'),\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schema)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Give 3 fact about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# prompt = template.invoke({'topic':'black hole'})\n",
    "# output = model.invoke(prompt)\n",
    "# result = parser.parse(output.content) # OR by chain\n",
    "\n",
    "chain = template | model | parser\n",
    "result = chain.invoke({'topic':'black hole'})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f50c5c",
   "metadata": {},
   "source": [
    "### PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=\"google/gemma-2-2b-it\", task=\"text-generation\")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "class Person(BaseModel):\n",
    "\n",
    "    name: str = Field(description='Name of the person')\n",
    "    age: int = Field(gt=18, description='Age of the person')\n",
    "    city: str = Field(description='Name of the city the person belongs to')\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Generate the name, age and city of a fictional {place} person \\n {format_instruction}',\n",
    "    input_variables=['place'],\n",
    "    partial_variables={'format_instruction':parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | model | parser\n",
    "final_result = chain.invoke({'place':'south-african'})\n",
    "\n",
    "print(final_result) # validated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
